<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深度解读代码生成模型 StarCoder VS CodeLlama | Amelia</title><meta name="author" content="Amelia Yin"><meta name="copyright" content="Amelia Yin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="2023 年可以称之为大模型元年，也是 AI 模型在开源历史上最受关注的一年。各大企业、机构、高校纷纷发布了自研大模型，展示多年来 AI 能力的积累和从量变到质变的过程。大模型的出现突破了许多人对 NLP、对 AI 算法的过往认识，越来越多的人开始相信大模型能够很好的理解人类自然语言并与人类交互，人工智能离我们更近了一大步。当然，大模型的能力不仅如此，它甚至可以读懂程序员以及机器生成的代码。代码数">
<meta property="og:type" content="article">
<meta property="og:title" content="深度解读代码生成模型 StarCoder VS CodeLlama">
<meta property="og:url" content="http://example.com/2024/01/11/240111/index.html">
<meta property="og:site_name" content="Amelia">
<meta property="og:description" content="2023 年可以称之为大模型元年，也是 AI 模型在开源历史上最受关注的一年。各大企业、机构、高校纷纷发布了自研大模型，展示多年来 AI 能力的积累和从量变到质变的过程。大模型的出现突破了许多人对 NLP、对 AI 算法的过往认识，越来越多的人开始相信大模型能够很好的理解人类自然语言并与人类交互，人工智能离我们更近了一大步。当然，大模型的能力不仅如此，它甚至可以读懂程序员以及机器生成的代码。代码数">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/avatar.jpg">
<meta property="article:published_time" content="2024-01-10T16:00:00.000Z">
<meta property="article:modified_time" content="2024-05-17T09:24:30.765Z">
<meta property="article:author" content="Amelia Yin">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="CodeG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/avatar.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/01/11/240111/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度解读代码生成模型 StarCoder VS CodeLlama',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-05-17 17:24:30'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Amelia"><span class="site-name">Amelia</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度解读代码生成模型 StarCoder VS CodeLlama</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-10T16:00:00.000Z" title="发表于 2024-01-11 00:00:00">2024-01-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-17T09:24:30.765Z" title="更新于 2024-05-17 17:24:30">2024-05-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="深度解读代码生成模型 StarCoder VS CodeLlama"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>2023 年可以称之为大模型元年，也是 AI 模型在开源历史上最受关注的一年。各大企业、机构、高校纷纷发布了自研大模型，展示多年来 AI 能力的积累和从量变到质变的过程。大模型的出现突破了许多人对 NLP、对 AI 算法的过往认识，越来越多的人开始相信大模型能够很好的理解人类自然语言并与人类交互，人工智能离我们更近了一大步。当然，大模型的能力不仅如此，它甚至可以读懂程序员以及机器生成的代码。代码数据是高度逻辑思维下的产物，是信息时代的精粹，是计算机领域人与人之间、人与机器之间的连接纽带。解决好大模型的代码能力，就能解决好大模型与所有计算机信息系统的交互问题。目前大语言模型 (LLM) 已经发展到了熟练掌握自然语言的程度，通过在特定领域的数据集上进行训练，可以进一步衍生出各种下游任务。其中一个应用便是基于代码训练的 LLMs (Code LLMs)，通过与模型的交互，由自然语言指令进行程序合成、代码补全、代码调试和生成文档。生成式代码大模型可以提高代码工作者的工作效率，对未来的计算机产业产生深远影响。目前，微软的 Copilot 已经吸引了超过 100 万的代码开发人员，GitHub 报告指出，这些开发者依赖 Copilot 编写了 35% 的代码。</p>
<p>在这种背景下，HuggingFace BigCode 团队和 Meta 团队也分别推出了自己的开源代码大模型：StarCoder 和 CodeLlama。本文我们会详细介绍这两种模型的技术细节，横向对比两类模型在实际应用（题解生成、代码补全、单元测试）中的生成效果。</p>
<h1 id="深入了解-StarCoder-模型"><a href="#深入了解-StarCoder-模型" class="headerlink" title="深入了解 StarCoder 模型"></a><strong>深入了解 StarCoder 模型</strong></h1><p>StarCoder系列模型是由 Hugging Face 和 ServiceNow 共同领导的开源项目组织 BigCode 所建立，该组织致力于开发代码大模型 (CodeLLM)，于2023年5月推出了 StarCoder 和 StarCoderBase 两个模型，具有 8K 上下文长度、填充 (infilling) 功能和快速大批量推理能力。其中 StarCoderBase 使用 1 万亿个 token 进行训练，训练数据来源于 GitHub 上的授权数据，其中包括 86 种编程语言、Jupyter笔记本、GitHub issues和 commits，StarCoder 是使用35B的python token数据在基础模型 StarCoderBase 上微调而成。接下来让我们来逐步解密Starcoder模型的产生细节吧。</p>
<h2 id="01-解密-StarCoder-预训练数据管理和清洗"><a href="#01-解密-StarCoder-预训练数据管理和清洗" class="headerlink" title="01 解密 StarCoder 预训练数据管理和清洗"></a><strong>01 解密 StarCoder 预训练数据管理和清洗</strong></h2><p>BigCode 团队对 StarCoder 模型的训练数据做了非常清晰介绍及解释。这一部分我们会讨论 starcoder 在训练数据集的选择，不同代码数据的清洗处理方式，不同语言在训练数据集中的采样分布以及数据去重等内容。</p>
<h3 id="训练集"><a href="#训练集" class="headerlink" title="训练集"></a><strong>训练集</strong></h3><p>StarCoder 和 StarCoderBase 的训练集来自于公开数据集 The Stack v1.2 (<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/bigcode/the-stack">https://huggingface.co/datasets/bigcode/the-stack</a>)，其中包含 6TB 的授权数据，覆盖 358 种编程语言。StarCoder 团队经过启发式过滤、人工检查筛选、清洗等处理之后还剩余 783GB 的代码数据，包含 86 种编程语言，其中有 54GB 的 github issues 数据和 13GB jupyter notebook 脚本和 text-code 数据对，32GB 的 github commits 数据，总计将近 250B tokens。Bigcode 组织秉承开源精神公开了清洗后的数据集——Starcoderdata (<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/bigcode/starcoderdata">https://huggingface.co/datasets/bigcode/starcoderdata</a>)。</p>
<h3 id="数据选择"><a href="#数据选择" class="headerlink" title="数据选择"></a><strong>数据选择</strong></h3><p>StarCoder 团队从 The Stack 公开代码数据集的 358 种编程语言中选择了 86 种语言，并根据文件扩展名对编程语言分类，保留超过 500 MB，以及在 Github 2.0 或 2022 年 12 月 TIOBE 编程语言流行度指数上排名前 50 的语言，其中也包含了像 JSON 和 YAML 等数据文件，但对这方面的数据做了数量限制以平衡整个数据集的分布，并且剔除了配置语言 (如 Nix、Puppet 等)以及已经被时代废弃的的语言如 ActionScript。Starcoder 团队对不同类型的编程语言有个性化的处理逻辑，下文我们进一步分析了具体的处理方法。</p>
<p><strong>人工检查</strong></p>
<p>为了筛选出高质量的数据，StarCoder 团队通过人工检查的方式进行数据处理。首先从 The Stack 数据集中为每种编程语言随机选择 30,000 个文件，按扩展名对它们进行分类，并为每个扩展名保留最多 1,000 个文件。StarCoder 团队招募了一些标注员，指导他们抽样检查其中 50-100 个文件，确认是否是人类编写的正常代码，而不是文本、数据或一长串自动生成的代码。标注员需要确定是否应该对某一种语言使用字母数字过滤器 (要求有超过 25% 的字母数字符号) 和长行文本过滤器 (要求每行少于 1,000 个字符) 。</p>
<p><strong>XML 过滤器</strong></p>
<p>StarCoder 团队实现了一个简单的 XML 过滤器，用于检查文件的前 100 个字符中是否存在<code>&lt;?xml version=</code>。使用此工具对除了 XSLT （XSLT 语法与 XML 相同）之外的所有编程语言进行过滤。</p>
<p><strong>字母过滤器</strong></p>
<p>在代码数据场景中，像 MATLAB 类型的数据中通常有大量包含大型 tensor 的数据文件。为了过滤这种文件，StarCoder 团队开发了一个字母过滤器，可以删除字母字符少于 25% 的文件。但是这个过滤器在某些编程语言 (例如汇编语言) 中的误报率很高，所以团队最后决定重点关注检测次数最多的 25 个编程语言，并通过人工确认是否应该在此编程语言上应用字母过滤器。</p>
<p><strong>HTML 过滤器</strong></p>
<p>Starcoder 团队设计了一个 HTML 过滤器，只保留可见文本至少占 20% 且最小长度为 100 个字符的文件。这样可以避免保留下来的文档中出现过多 HTML 标签和链接。</p>
<p><strong>JSON and YAML 清理</strong></p>
<p>对于 YAML 文件，保留 50-5000 个字符、平均行长度小于 100、最大行长度小于 1000，并且字母字符超过 50% 的文件，这使得 YAML 文件减少了大约 20% 的文件和 90% 的体积。类似地，对于 JSON 文件，保留 50-5000 个字符且超过 50% 字母字符的文件，这使得 JSON 文件减少了大约 70% 的文件和 98% 的体积。</p>
<p><strong>Jupyter</strong></p>
<p>Jupyter 笔记本的数据主要包括两种：</p>
<p>1）第一种是 Jupyter scripts，使用 Jupytext 工具将笔记本转换为 Jupyter scripts。从每个笔记本的 meta 信息里可以获取编程语言类别，Jupytext 可以根据这个信息识别不同语言的代码段。但是有超过 30,000 个笔记本缺少编程语言信息，所以 StarCoder 团队结合使用了 Guesslang 工具进行识别，该工具使用了机器学习方法识别代码片段所属的编程语言。</p>
<p>2）第二种是 Jupyter – structured，创建这类数据首先需要过滤掉不包含 Python 代码也不包含 Markdown 文本的笔记本，只有 meta 信息中明确标记为“Python”语言的笔记本会被留下。在每个笔记本内部，连续的 Markdown 块或代码块会合并为一个大的 Markdown 或代码块，最终得到连续的按时间顺序排列的代码-文本对。包含 Python 代码的代码块与 Markdown 文本紧邻，形成天然的指令对。</p>
<p><strong>GitHub issues</strong></p>
<p>从 The Stack v1.2 中收集来自 GitHub  issues 和 pull request 的自然语言对话，每个对话都包含一系列操作事件，例如开启问题、创建评论和关闭问题。每个事件都包括作者的用户名、消息、操作和创建日期。删除了用户通过电子邮件回复问题时自动生成的文本，也删除了少于 200 个字符的问题。删除机器人生成的评论，机器人生成的问题往往很长，并且包含大量日志和链接。按照参与对话的用户数量进行过滤，保留两个及两个以上数量用户的对话，最后使用 fasttext 工具过滤掉非英语的问题。</p>
<p><strong>Git commits</strong></p>
<p>由于这部分数据会包含用户提交的更改前以及更改后大量重复的代码片段，为了避免模型在学习相同内容上花费过多的计算资源，StarCoder 团队决定仅在 20% 的时间内使用完整文件，而在其余 80% 的时间内，对第一个和最后一个更改行周围的 0 到 32 行之间的窗口进行采样。</p>
<h3 id="去重处理"><a href="#去重处理" class="headerlink" title="去重处理"></a><strong>去重处理</strong></h3><p>Starcoder 团队在去重方法上首先采用了 MinHash，即计算所有代码文件的 MinHashes，然后使用本地敏感哈希 (LSH) 将相似的代码文件映射到同一存储桶。对所有编程语言和 Jupyter 笔记本都进行了去重，但是由于时间限制没有应用于 Git commits。不过 Git commits 不太可能发现重复项，所以 StarCoder 团队认为这部分数据不需要进行去重。</p>
<h3 id="数据集的权重"><a href="#数据集的权重" class="headerlink" title="数据集的权重"></a><strong>数据集的权重</strong></h3><p>一般地，数量比较多的数据基本都来自于相对更流行的编程语言，StarCoder 团队认为不需要对现有的数据分布进行大幅度的重新加权，在训练过程中按照数据的原始分布比例对数据源进行采样，这样训练出来的模型也能够照顾到这些流行语言的用户。但在这其中 JSON、YAML 和 CSS 格式是例外，因为 LLM 应该重点学习这些语言的数据格式，而不是浪费计算资源来记住其中的数据。所以 StarCoder 团队重新分配了这几种语言的大小，JSON 和 YAML 为 1 GB，CSS 为 3 GB。</p>
<h2 id="02-StarCoder-模型训练方法和技巧"><a href="#02-StarCoder-模型训练方法和技巧" class="headerlink" title="02 StarCoder 模型训练方法和技巧"></a><strong>02 StarCoder 模型训练方法和技巧</strong></h2><p>在上文我们了解了预训练数据的准备流程后，我们再来了解一下StarCoder模型在训练过程当中运用的具体方法，涉及数据格式构造、Tokenizer选择以及模型框架搭建。</p>
<h3 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a><strong>数据格式</strong></h3><p>在这部分我们会详细介绍 StarCoder 团队如何构造预训练数据的格式，其中<token>指的是特殊令牌 (special token)，metadata 和 data 则是数据字段的占位符。</token></p>
<p><strong>代码</strong></p>
<p>代码部分的数据一共包含三个 meta 信息和代码正文 (code)，meta 信息分别为存储库名称 (reponame)、文件名 (filename) 以及星级 (stars)，格式为<code>&lt;reponame&gt;reponame&lt;filename&gt;filename&lt;gh_stars&gt;stars\ncode&lt;|endoftext|&gt;</code></p>
<p>StarCoder 将 meta 信息添加到代码文件的上下文中。为了使模型能够在没有 meta 信息的情况下进行推理，这三类 meta 信息被独立添加前缀，概率均为 0.2。将中间填充变换 (Fill-in-the-Middle Transformation, FIM) 以字符级应用于源代码文件，覆盖率为 0.5，其中一半使用前缀-后缀-中间 (prefix-suffix-middle, PSM) 模式，一半使用后缀-前缀-中间 (suffix-prefix-middle, SPM) 模式。</p>
<p><strong>GitHub issues</strong></p>
<p>GitHub 上面的每个仓库 (repository) 都有“错误跟踪器”，称为“issues”，供仓库创建者、贡献者、使用者或任何想参与讨论的人在使用开源项目发现代码 bug 或出现问题时使用。一个 GitHub issues 实例见下图。</p>
<p><img src="/2024/01/11/240111/Xnc3bBSSyoizrAxZvkNc4fDanzh.png"></p>
<p>这部分数据被处理为以下格式：<code>&lt;issue_start&gt;Title:title\nusername_id0:comment0&lt;issue_comment&gt;username_id1:comment1 ... &lt;issue_closed(optional)&gt; &lt;|endoftext|&gt;</code></p>
<p>通过<code>&lt;issue_comment&gt;</code>来分离一系列评论，并在评论前放置一个匿名化的说话人标识符。为了区分不同对话轮，使用<code>comment1</code> ，<code>id1</code>分别指代第二个评论和与之对应的匿名说话人 id。</p>
<p><strong>Jupyter</strong></p>
<p>1）Jupyter – scripts</p>
<p>Jupyter - scripts 是由 Jupyter 笔记本转换为脚本文件而得。</p>
<p>这部分的数据结构与代码部分结构相同：<code>&lt;reponame&gt;reponame&lt;filename&gt;filename&lt;gh_stars&gt;stars\ncode&lt;|endoftext|&gt;</code></p>
<p>2）Jupyter – structured</p>
<p>Jupyter – structured 部分的数据包含 Python 代码块和与之相邻的 Markdown 文本，形成天然的指令对，其格式为 <code>&lt;jupyter_start&gt;&lt;jupyter_text&gt;text0&lt;jupyter_code&gt;code0&lt;jupyter_output&gt;output0&lt;jupyter_text&gt; ... &lt;|endoftext|&gt;</code></p>
<p>解析出来的 Jupyter 笔记本以文本 (text)、代码 (code) 和输出 (output) 为链，以<code>&lt;jupyter_start&gt;</code>标记每一对<code>text-code-output</code>的开始，<code>text2</code>，<code>code2</code>，<code>output2</code>用来指代笔记本中的第 3 个三元组。</p>
<p><strong>Git commits</strong></p>
<p>在 Git 管理中，“commits”（提交）是一种保存项目更改的方式。每个提交都代表了项目在某一时间点的状态，包含了文件的变更信息。提交可以记录编程者对项目所做的修改，包括添加、删除或修改文件。一个 Git commit 实例见下图。</p>
<p><img src="/2024/01/11/240111/WsdjbYxOsoumsXxskn6cPOMYn6f.png"></p>
<p>这部分数据被处理为以下格式：<code>&lt;commit_before&gt;code_before&lt;commit_msg&gt;message&lt;commit_after&gt;code_after&lt;|endoftext|&gt;</code></p>
<p>以<code>commit_before</code>、<code>commit_msg</code>和<code>commit_after</code>为链。以 20% 的概率使用完整文件，不使用完整文件的时候就在变化的行周围使用一个小窗口 (0 ~ 32 行)。</p>
<h3 id="分词器"><a href="#分词器" class="headerlink" title="分词器"></a><strong>分词器</strong></h3><p>StarCoder 使用 Hugging Face Tokenizers 库训练一个字节级的字节对编码 (BBPE)的分词器，共 49152 个 token。</p>
<h3 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a><strong>模型框架</strong></h3><p>StarCoder 采用与 SantaCoder 架构相同的 15.5 B 参数模型。它是一个 decoder-only 的 Transformer，运用多查询注意力 (MQA)，学习绝对位置 embedding。将中间填充 (Fill-in-the-Middle, FIM) 变换应用在训练数据上，并使用 Flash Attention 来加速注意力计算并减少其内存占用，使得上下文长度可以扩展到 8K。</p>
<h2 id="03-StarCoder-模型评测详解"><a href="#03-StarCoder-模型评测详解" class="headerlink" title="03 StarCoder 模型评测详解"></a><strong>03 StarCoder 模型评测详解</strong></h2><p>StarCoder 团队列出了包括 StarCoder 和 StarCoderBase 等一些模型在 Human Eval、MBPP 和 DS - 1000  benchmark 上的 Python 语言得分，同时也评测了多种编程语言的 benchmark。</p>
<h3 id="StarCoder-Python-Evaluation"><a href="#StarCoder-Python-Evaluation" class="headerlink" title="StarCoder: Python Evaluation"></a><strong>StarCoder: Python Evaluation</strong></h3><p><strong>HumanEval 和 MBPP 基准</strong></p>
<p>StarCoder 团队将 StarCoder 和 StarCoderBase 与几种模型在 HumanEval 和 MBPP 基准上进行了比较。测试结果如下图。</p>
<p><img src="/2024/01/11/240111/B3v0bxCmwob94axlaeYcBa9Mnlg.jpeg"></p>
<p>可以看到，在这两个基准上进行测试的结果中，StarCoder 是表中分数最高的开源模型。尽管 StarCoder 的规模比 PaLM，LaMDA 和 LLaMA 等模型的规模小很多，但是其分数却优于这些规模较大的模型。StarCoderBase 对 Python 的理解也很出色，与 CodeGen-16B-Mono (一个在 Python 上微调的类似大小的开源模型) 不相上下。同时，StarCoder 也优于 OpenAI 推出的 code-cushman-001 (12B) 模型。</p>
<p><strong>DS-1000 python 数据科学基准</strong></p>
<p>HumanEval 和 MBPP 的局限性在于它们只包含编程题，而编程题中的代码并不能代表大多数程序员所写的代码。相比之下，DS-1000 更贴近实际应用中会出现的代码，它涵盖了 7 个 Python 库 (如 NumPy、Pandas 等) 的相关问题和答案。测试结果如下图。</p>
<p><img src="/2024/01/11/240111/BtKfbnvB2omCiXx3f39ccaefnkb.png"></p>
<p>从 DS-1000 基准来看，StarCoder 在数据科学问题上的表现优于 StarCoderBase，二者同时远远优于其他所有模型。与此同时，StarCoder 团队还有进一步的发现：在 HumanEval 和 MBPP 基准测试中性能表现好的模型并不一定能在 DS-1000 基准测试中拥有同样好的表现。例如，CodeGen-Mono 在 HumanEval 和 MBPP 上的性能略优于 code-cushman-001 和 StarCoder 模型，但在 DS-1000 上明显较差。这证明了在一系列基准上评估模型的重要性。</p>
<p><strong>The ODEX 开放域编程基准 (Open-Domain Coding Benchmark)</strong></p>
<p>上文评估要么侧重于封闭域 (例如内置 Python 函数，如 MBPP 和 HumanEval 基准) ，要么侧重于特定域 (例如数据科学，如 DS-1000 基准) 。为了更广泛评估模型在 Python 库上生成代码的能力，StarCoder 团队使用 ODEX 基准进一步进行测试，测试集包含 505 个开放域和 440 个封闭域 Python 编程问题，采用四种自然语言——英语、西班牙语、日语和俄语——基于测试用例进行评估。测试结果如下。</p>
<p><img src="/2024/01/11/240111/BQehbkQNFoDvGkxRRcKcTiJsnBg.png"></p>
<p>StarCoder 在开放域编程方面远远优于所有其他模型。StarCoderBase 紧随其后，其性能也优于其他所有模型，甚至在 ODEX 英语子集中优于 StarCoder，但在其他语言中稍微落后。</p>
<h3 id="StarCoder-and-StarCoderBase-Multi-Language-Evaluation"><a href="#StarCoder-and-StarCoderBase-Multi-Language-Evaluation" class="headerlink" title="StarCoder and StarCoderBase: Multi-Language Evaluation"></a><strong>StarCoder and StarCoderBase: Multi-Language Evaluation</strong></h3><p>尽管 StarCoder 是在 Python 上进行微调得到的，但它仍是一个非常强大的多语言代码 LLM，甚至在某些语言上的表现优于 StarCoderBase。</p>
<p><strong>使用 MultiPL-E 在 19 种编程语言上进行评估</strong></p>
<p>StarCoder 团队使用 MultiPL-E 评测集评估 StarCoder 将自然语言转换为多种编程语言的能力。MultiPL-E 将 HumanEval 和 MBPP 中的 Python 语言翻译成了其他 18 种编程语言。各模型在总共 19 种编程语言上的表现如下图。</p>
<p><img src="/2024/01/11/240111/RPgobITucoSR6rxEFXNciZGNn6f.png"></p>
<p>从所有 19 种编程语言测试的结果整体来看，StarCoderBase 的性能优于其他开源模型，有时甚至表现出超过 2 倍的性能。除了少部分语言，StarCoderBase 与 code-cushman-001 的表现不相上下。与此同时，尽管 StarCoder 是在 Python 上进行微调得到的，但其在大多数语言上的表现仍然很出色，优于其他开源模型，甚至 StarCoder 在某些语言上的表现要略胜于 StarCoderBase。</p>
<p><strong>“Asleep at the Keyboard” 安全基准</strong></p>
<p>代码大模型的一个局限性在于它们有可能生成具有安全漏洞的代码。Asleep at the Keyboard 基准包含 89 个与安全相关的情景以供测试。测试结果如下。</p>
<p><img src="/2024/01/11/240111/Sg1hb39MAo5uxyxjrQacSWIHndZ.png"></p>
<p>StarCoderBase 的代码有效率最高。InCoder-6B 的不安全代码生成率略低，但这可能是由于其代码有效率较低。在有效代码超过 95% 的模型中，StarCoder 的不安全率最低。</p>
<p><strong>Fill in the Middle 基准</strong></p>
<p>StarCoder 模型支持中间填充 (Fill in the Middle, FIM) ，即模型根据前后文 (prefix, suffix) 在插入点周围进行代码生成。StarCoder 团队根据 4 个 FIM 基准进行了测试。</p>
<p>(1) Python、Java 和 JavaScript 的单行填充</p>
<p><img src="/2024/01/11/240111/BqqKbVTVuoNnn7xzrCTc3tPHnRb.png"></p>
<p>StarCoderBase 的性能明显优于其他两个规模较小的模型。</p>
<p>(2) Python 返回类型预测</p>
<p><img src="/2024/01/11/240111/IQzWbPAsuomcNyxKN6LcFjTpnhc.png"></p>
<p>StarCoder 和 StarCoderBase 在 Python 返回类型预测方面优于其他模型。但是由于该评测集中的函数取自 GitHub 存储库，因此它们可能与 SantaCoder 和 StarCoder 模型的训练数据重叠。</p>
<p>(3) TypeScript 类型预测</p>
<p><img src="/2024/01/11/240111/N8KSbpc5KoUo3GxOZmDcVZ1sncg.png"></p>
<p>StarCoderBase 的表现优于 InCoder。</p>
<p>(4) Python Docstring 生成</p>
<p><img src="/2024/01/11/240111/ZzSwb9fdnoZxTIxMPFKcHynHnJf.png"></p>
<p>StarCoder 和 StarCoderBase 在 docstring 生成方面有更好的表现。但此评估数据集与用于训练 SantaCoder 和 StarCoder 模型的数据之间可能存在重叠。</p>
<h2 id="04-StarCoder-如何保障数据隐私安全？"><a href="#04-StarCoder-如何保障数据隐私安全？" class="headerlink" title="04 StarCoder 如何保障数据隐私安全？"></a><strong>04 StarCoder 如何保障数据隐私安全？</strong></h2><p>StarCoder 采用一种个人身份信息修正 (Personally Identifiable Information redaction, PII redaction) 机制，从训练数据中删除个人身份信息。收集并标注 PII 数据集，将其用于 PII 检测模型的训练，最终得到 PII 检测模型。</p>
<h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a><strong>数据收集</strong></h3><p>Starcoder 团队召集了来自 35 个国家的 1,399 名众包人员来注释源代码中的 PII 数据集。PII 类型包括姓名、用户名、电子邮件、IP 地址、密钥、密码和用户名。</p>
<h3 id="数据组成"><a href="#数据组成" class="headerlink" title="数据组成"></a><strong>数据组成</strong></h3><p>PII 数据集包括 12,000 个文件，31 种编程语言，每个文件包含大约 50 行代码。在这其中，为了增加密钥、IP 地址等稀有 PII 类型的代表性，团队预过滤了 7100 个文件，使用 <a target="_blank" rel="noopener" href="https://github.com/Yelp/detect-secrets">detect-secrets</a> 工具以及正则表达式来检测电子邮件、IPv4 和 IPv6 地址。为了防止标注结果过度依赖检测工具，还从数据集中随机选择了没有进行过预过滤的 5100 个文件。</p>
<h3 id="区分-PII"><a href="#区分-PII" class="headerlink" title="区分 PII"></a><strong>区分 PII</strong></h3><p>根据 PII 出现的上下文来区分 PII 是否需要被掩盖，包括是否存在于代码的许可证头中、是否被用作占位符以及是否构成机密数据。之所以要进行这样的区分，是因为许可证中的 PII 通常是由作者自愿提供的，而占位符也没有私密性，所以出现在这两处的 PII 都不需要被掩盖。只需将这种区分应用于姓名、电子邮件、用户名。</p>
<h3 id="质量评估"><a href="#质量评估" class="headerlink" title="质量评估"></a><strong>质量评估</strong></h3><p>Starcoder 团队通过人工检查了 300 个包含各种 PII 类型的文件，并计算了每种类型的查全率和查准率。标注用户名经常出现错误标注，所以 StarCoder 团队决定将该类别排除在 PII 检测模型训练之外。</p>
<h3 id="StarEncoder"><a href="#StarEncoder" class="headerlink" title="StarEncoder"></a><strong>StarEncoder</strong></h3><p>StarCoder 为了 PII 检测专门训练了一个 encoder-only 模型，可以有效地针对代码和文本相关的任务进行微调。使用 BERT 中的基于掩盖的语言模型 (Masked Language Model, MLM) 和下句预测任务 (Next Sentence Prediction, NSP)，预测输入句子中被盖住的 token 以及一对输入是否是文章中前后文。</p>
<h3 id="PII-检测模型"><a href="#PII-检测模型" class="headerlink" title="PII 检测模型"></a><strong>PII 检测模型</strong></h3><p>PII 检测的总体思路是命名实体识别 (Named Entity Recognition, NER)。用标注过的 PII 数据集对 StarEncoder 进行微调。在模型顶部添加了一个线性层作为分类头，有 6 个目标类：姓名、电子邮件、密钥、密码、IP 地址和用户名。</p>
<h3 id="PII-占位符"><a href="#PII-占位符" class="headerlink" title="PII 占位符"></a><strong>PII 占位符</strong></h3><p>占位符分别有<code>&lt;NAME&gt;</code>,<code>&lt;EMAIL&gt;</code>,<code>&lt;KEY&gt;</code>,<code>&lt;PASSWORD&gt;</code>。</p>
<p>至于 IP 地址的占位，从 5 个合成出来的私有 IP 地址中随机选择一个同类 IP 地址进行替换。</p>
<h1 id="深入了解-Code-Llama-模型"><a href="#深入了解-Code-Llama-模型" class="headerlink" title="深入了解 Code Llama 模型"></a><strong>深入了解 Code Llama 模型</strong></h1><p>2023 年 08 月，Meta 发布了 Code Llama，它是基于 Llama 2 衍生出来的代码生成模型，在开源模型中具有先进的填充能力、长上下文输入能力以及在编程任务中的 zero-shot 能力。它可以降低代码工作的门槛，辅助开发人员写出完善且高质量的代码，成为生产力工具和教育工具。</p>
<p>Code Llama 系列模型 有多个版本：</p>
<ul>
<li>基础模型（Code Llama）</li>
<li>Python 专业化版本（Code Llama - Python）</li>
<li>指令跟随模型（Code Llama - Instruct）</li>
</ul>
<p><img src="/2024/01/11/240111/GxRKbBAZRo9dD6xgozxcTnRVnMd.png"></p>
<p>每个版本都分别有三个尺寸，参数量各为 7B、13B 和 34B，并支持多种编程语言。所有模型都是在 16k token 序列上进行训练的，并稳定支持最高 10 万个 token 的上下文生成。7B 和 13B 的 Code Llama 以及 Code Llama - Instruct 支持基于周围内容的填充。7B 模型适合在单个 GPU 上运行，34B 模型则提供了更好的编程辅助效果，但是相比较之下速度较慢。在面对比如实时代码补全这种低延迟轻量任务，更适合使用速度较快量级的 7B 和 13B 模型。</p>
<p>Code Llama 在多个代码基准测试中达到了开放模型中的最先进性能，分别在 HumanEval 和 MBPP 上取得了高达 53% 和 55% 的分数 (优于 Llama2 70B)，在 MultiPL-E 评测中的精度优于所有开源模型。</p>
<p>Meta 的方法是通过应用一系列训练和微调逐步提高 Llama 2 模型的能力并逐渐将其专业化。在预算相同的情况下，使用 Llama 2 初始化模型优于仅针对代码数据进行预训练的相同架构的模型。</p>
<h2 id="01-解密-CodeLlama-微调方案"><a href="#01-解密-CodeLlama-微调方案" class="headerlink" title="01 解密 CodeLlama 微调方案"></a><strong>01 解密 CodeLlama 微调方案</strong></h2><p>正如上文所介绍，CodeLlama通过一系列的训练和微调逐步提升模型能力，包括填充、长文本微调和指令微调。</p>
<h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a><strong>填充</strong></h3><p>代码填充是指在给定周围上下文的情况下预测缺失部分。其应用包括代码补全、类型推断和生成代码内文档 (比如 docstring) 。</p>
<p>按照因果掩码 (Causal Masking) 的概念来训练填充模型，序列的一部分被移到末尾，重新排序后的序列进行自回归预测。将训练数据在字符级别分成前缀 (prefix)、中间部分 (middle) 和后缀 (suffix)。分割位置在文档长度上从均匀分布中独立采样，采样概率为 0.9。随机将一半的分割以前缀-中间-后缀 (Prefix-Suffix-Middle, PSM) 格式进行格式化，将另一半以后缀-前缀-中间 (Suffix-Prefix-Middle, SPM) 格式进行格式化。</p>
<h3 id="长文本微调"><a href="#长文本微调" class="headerlink" title="长文本微调"></a><strong>长文本微调</strong></h3><p>Code Llama 创建了一个长上下文微调 (LCFT) 阶段，模型的输入长度为 16,384 个 token 的序列，比 Llama2 和初始代码训练阶段所使用的 4,096 个 token 要多。通过将处理长序列的训练时间限制在微调阶段，可以在不显著增加模型训练成本的情况下获得处理长距离注意的能力。</p>
<p>在使用旋转位置嵌入时，旋转频率的设置十分重要。Code Llama 改变了旋转位置嵌入 (rotary position embedding, RoPE) 的基础周期：</p>
<p><img src="/2024/01/11/240111/J0bnbceXCoNzJFxR4R2cKicHnQh.png"></p>
<p>其中 n 表示位置，d 表示 embedding 维度，旋转频率的计算公式为 <code>θi = θ^(−2i/d)</code>。</p>
<p>基础周期 θ 从 10,000 增加到 1,000,000，这样可以处理更长的序列并减少对短距离注意力的偏见。实验证实，Code Llama 模型不仅在微调过程中的序列长度内有效，而且还显示出外推能力，在 100,000 个 token 的超长序列上表现稳定。</p>
<h3 id="指令微调"><a href="#指令微调" class="headerlink" title="指令微调"></a><strong>指令微调</strong></h3><p>指令微调主要有三种数据集来源，专用数据集、self-instruct数据集以及代码与自然语言组合数据集。</p>
<p><strong>(1) 专有数据集</strong></p>
<p>采用 Llama2 中的 RLHF V5 版本数据集，该数据集包括数千个监督微调 (Supervised Fine-Tuning) 示例和数百万个拒绝采样 (Rejection Sampling) 示例。每个示例由用户 (user) 和助手 (assistant) 之间的多轮对话组成。</p>
<p><strong>(2) Self-instruct 数据集</strong></p>
<p>Meta 构建了约 14,000 个问题-测试-解决方案 (question-tests-solution) 三元组。构建方法如下：</p>
<ul>
<li><p>通过提示 Llama 2 70B，生成 62,000 个面试形式的编程问题；</p>
</li>
<li><p>通过删除完全重复项来去除重复的问题集，从而剩下约 52,000 个问题；</p>
</li>
<li><p>对于每一个问题：</p>
<ul>
<li>通过提示 Code Llama 7B 生成单元测试 (unit tests)；</li>
<li>通过提示 Code Llama 7B 生成十个 Python 解决方案；</li>
<li>对这十个解决方案进行单元测试，将通过测试的第一个解决方案（及其相应的问题和测试）添加到 Self-instruct 数据集。</li>
</ul>
</li>
</ul>
<p>使用 Code Llama 7B 生成单元测试和解决方案的原因是它所消耗的计算资源更低，而且效果已经令人满意。</p>
<p><strong>(3) 代码和自然语言组合数据集</strong></p>
<p>为了防止模型在编程和语言理解能力方面出现退化，Code Llama - Instruct 使用代码数据集 (6%) 和自然语言数据集 (2%) 的配比的方式来进行训练。</p>
<h2 id="02-CodeLlama-的训练细节详解"><a href="#02-CodeLlama-的训练细节详解" class="headerlink" title="02 CodeLlama 的训练细节详解"></a><strong>02 CodeLlama 的训练细节详解</strong></h2><h3 id="优化参数设置"><a href="#优化参数设置" class="headerlink" title="优化参数设置"></a><strong>优化参数设置</strong></h3><ul>
<li><p>使用 AdamW 优化器，其中 β1 设置为 0.9，β2 设置为 0.95.</p>
</li>
<li><p>具有 1000 个预热步骤的余弦学习率，并将最终学习率设置为峰值学习率的 1&#x2F;30.</p>
</li>
<li><p>batch size 是 4M 个 token，每 4, 096 个 token 为一个序列。</p>
</li>
<li><p>Meta 团队并没有选择普遍使用的标准做法——在微调阶段使用比预训练阶段更低的学习率，而是保留 Llama 2 基础模型的原始学习率，经过试验发现这种做法可以获得最佳结果。</p>
<ul>
<li>将这种做法应用于 13B 和 34B 模型，学习率分别设置为 3e^−4 和 1.5e^−4.</li>
<li>对于 python 微调，将初始学习率设置为 1e^−4.</li>
</ul>
</li>
<li><p>Code Llama - Instruct 的 batch 大小是 524,288 个 token，总计 5B 个 token。</p>
</li>
</ul>
<h3 id="长上下文微调-Long-context-fine-tuning-LCFT"><a href="#长上下文微调-Long-context-fine-tuning-LCFT" class="headerlink" title="长上下文微调 (Long context fine-tuning, LCFT)"></a><strong>长上下文微调 (Long context fine-tuning, LCFT)</strong></h3><ul>
<li>使用 2e^−5 的学习率，16,384 的序列长度，并使用 θ &#x3D; 10^6 的基值来重置 RoPE 频率。</li>
<li>对于 7B 模型和 13B 模型，batch 大小设置为 2M 个 token；对于 34B 模型，batch size 设置为 1M 个 token。</li>
<li>默认情况下，训练有 10,000 个梯度步骤。有些时候会出现下游性能不稳定的情况，因此将 34B 模型的梯度步数设置为 11,000，将 Code Llama 7B 的梯度步数设置为 3,000。</li>
</ul>
<h2 id="03-CodeLlama-模型评测"><a href="#03-CodeLlama-模型评测" class="headerlink" title="03 CodeLlama 模型评测"></a><strong>03 CodeLlama 模型评测</strong></h2><p>Code Llama 34B 模型在 HumanEval 评测中取得了 53.7% 的通过率，在 MBPP 评测中的通过率则达到了 56.2%，接近 Chatgpt 的水平。</p>
<p><img src="/2024/01/11/240111/Rgoxb4WPAo9583xqG2Occ2Chnng.png"></p>
<h2 id="04-CodeLlama-如何保障模型安全？"><a href="#04-CodeLlama-如何保障模型安全？" class="headerlink" title="04 CodeLlama 如何保障模型安全？"></a><strong>04 CodeLlama 如何保障模型安全？</strong></h2><p>Meta 分别从诚实性 (Truthfulness)、有害性 (Toxicity) 和偏见 (Bias) 的角度用三个基准对 CodeLlama 的安全性进行了评估。</p>
<p>诚实性：使用 Truthful QA 来衡量模型的事实性和常识性。Truthful QA 基准包括 817 个问题，涵盖健康、金融、法律和政治等主题。这其中的有些问题十分具有挑战性，甚至人类在没有依据或者认知上存在错误的情况下也会答错。</p>
<p>有害性：使用 ToxiGen 来量化一段话具有危害性的程度，以及带有何种程度的仇恨。ToxiGen 数据集包含 13 个少数群体的隐性有害和良性句子。使用默认的 ToxiGen 分类器来衡量大模型产生输出的有害性。</p>
<p>偏见：使用开放语言生成数据集 (Bias in Open-Ended Language Generation Dataset, BOLD) 来研究模型输出所包含的情感。</p>
<p>测试结果如下图。</p>
<p><img src="/2024/01/11/240111/IngTbJcOwoKVJYxbkJhcQtzXnWc.png"></p>
<p>可以看到，微调后的 Code Llama - Instruct 在诚实性和有害性方面的表现比预训练的 Code Llama 更好。所有版本的 Code Llama 有害内容占比都低至几乎为 0%，在所有进行比较的模型中有害性最低。与 Falcon 和 MPT 微调模型相比，微调后的 Code Llama 在有害性和诚实性方面均表现出次优水平，仅次于 Llama 2 Chat。除此之外，Code Llama - Instruct 在微调后也表现出正向情感的整体增加。</p>
<h1 id="StarCoder-和-CodeLlama-谁更适合作为基座模型？"><a href="#StarCoder-和-CodeLlama-谁更适合作为基座模型？" class="headerlink" title="StarCoder 和 CodeLlama 谁更适合作为基座模型？"></a><strong>StarCoder 和 CodeLlama 谁更适合作为基座模型？</strong></h1><p>StarCoder 和 CodeLlama 是两种训练路线的生成代码模型。</p>
<p>StarCoderBase 的预训练数据几乎全是代码数据，是从零开始预训练起来的代码生成模型；而 CodeLlama 是在 Llama2 通用模型上用代码数据微调而成。</p>
<p>在没有基于自然语言模型进行微调的基础上，StarCoder 也表现出了不凡的自然语言理解能力。除此之外，StarCoder 数据更加透明，利用 opt-out 机制让用户可以自行选择将自己仓库中的代码数据排除在数据集之外，数据安全性更高。而 CodeLlama 用来训练的数据以及对于数据进行处理的细节都没有公开，具有不确定性，用户无法清楚得知有哪些数据被包含在模型训练当中以及在这个过程中该团队是如何保障数据隐私安全的。虽然 CodeLlama 对模型进行了数据安全的基准测试，但是这些基准都有其局限性，所以并不能完全证明 CodeLlama 具有全面的数据安全性。</p>
<p>单一评测基准经常有“偏好”上的局限性，因此在多个基准上评估模型非常重要。StarCoder 团队在评测模型性能的时候就曾有发现：在 HumanEval 和 MBPP 基准测试中性能表现好的模型并不一定能在 DS-1000 基准测试中拥有同样好的表现。例如，CodeGen-Mono 在 HumanEval 和 MBPP 上的性能略优于 code-cushman-001 和 StarCoder 模型，但在 DS-1000 上明显较差。不同评测基准难免具有自己的“偏见”，导致了评测结果的片面，在做模型评测的时候应该运用多种品测基准，且要辩证地看待评测结果。</p>
<h2 id="01-StarCoder-与-CodeLlama-在-HumanEval-benchmark-上的评测对比"><a href="#01-StarCoder-与-CodeLlama-在-HumanEval-benchmark-上的评测对比" class="headerlink" title="01 StarCoder 与 CodeLlama 在 HumanEval benchmark 上的评测对比"></a><strong>01 StarCoder 与 CodeLlama 在 HumanEval benchmark 上的评测对比</strong></h2><p>OpenCSG 团队深入研究了 StarCoder 以及 CodeLlama 这两种不同的代码生成模型训练方法，并且对 StarCoder 和 CodeLlama 系列模型进行了评测复现，结果如下表。</p>
<table>
<thead>
<tr>
<th>模型名</th>
<th>HumanEval Python Pass@1mailto:Pass@1</th>
</tr>
</thead>
<tbody><tr>
<td>StarCoder</td>
<td>35.98%</td>
</tr>
<tr>
<td>StarCoderbase</td>
<td>31.72%</td>
</tr>
<tr>
<td>CodeLlama-7b</td>
<td>30.49%</td>
</tr>
<tr>
<td>CodeLlama-13b</td>
<td>35.98%</td>
</tr>
<tr>
<td>CodeLlama-34b</td>
<td>48.17%</td>
</tr>
<tr>
<td>CodeLlama-7b-Python</td>
<td>40.24%</td>
</tr>
<tr>
<td>CodeLlama-13b-Python</td>
<td>46.34%</td>
</tr>
<tr>
<td>CodeLlama-34b-Python</td>
<td>53.65%</td>
</tr>
</tbody></table>
<p>从结果可以看出，CodeLlama 在 HumanEval python pass@1 的表现是随着模型的参数量增加，评测分数也逐步提升，其分数甚至高于 starcoder 这样的优秀代码生成模型。OpenCSG 分析 StarCoder 分数不高的原因的是其对于 humanEval 这样做代码题解的评测问题上没有进一步的微调，它更加侧重于解决代码领域内通用的问题。而 CodeLlama 通过 Python 专有数据微调过的 codellama-13b-python 和 codellama-34b-python 模型在 HumanEval - Python 评测中的表现都要更好，由此可见专用数据对于模型在垂类领域能力提升至关重要。</p>
<h2 id="02-StarCoder-与-CodeLlama-在实际应用中的比较"><a href="#02-StarCoder-与-CodeLlama-在实际应用中的比较" class="headerlink" title="02 StarCoder 与 CodeLlama 在实际应用中的比较"></a><strong>02 StarCoder 与 CodeLlama 在实际应用中的比较</strong></h2><p>除了评测之外，OpenCSG 团队基于实际应用场景，在题解生成、代码补全和生成单元测试三个方面对 StarCoder 和 CodeLlama-13b 的生成结果进行了比较。</p>
<ol>
<li><strong>题解生成</strong></li>
</ol>
<p>(1) 生成快排算法</p>
<p>测试代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;path&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;path&quot;</span>, trust_remote_code=<span class="literal">True</span>, torch_dtype=torch.bfloat16).cuda()</span><br><span class="line">input_text = <span class="string">&quot;#write a quick sort algorithm&quot;</span></span><br><span class="line">inputs = tokenizer(input_text, return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line">outputs = model.generate(**inputs, max_length=<span class="number">128</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>

<p>CodeLlama-13b-hf 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#write a quick sort algorithm</span></span><br><span class="line"><span class="comment">#this is a recursive algorithm</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">quick_sort</span>(<span class="params">arr</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(arr) &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> arr</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pivot = arr[<span class="number">0</span>]</span><br><span class="line">        less = [i <span class="keyword">for</span> i <span class="keyword">in</span> arr[<span class="number">1</span>:] <span class="keyword">if</span> i &lt;= pivot]</span><br><span class="line">        greater = [i <span class="keyword">for</span> i <span class="keyword">in</span> arr[<span class="number">1</span>:] <span class="keyword">if</span> i &gt; pivot]</span><br><span class="line">        <span class="keyword">return</span> quick_sort(less) + [pivot] + quick_sort(greater)</span><br></pre></td></tr></table></figure>

<p>StarCoder 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#write a quick sort algorithm</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">quick_sort</span>(<span class="params">arr</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(arr) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> arr</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pivot = arr[<span class="number">0</span>]</span><br><span class="line">        less = [i <span class="keyword">for</span> i <span class="keyword">in</span> arr[<span class="number">1</span>:] <span class="keyword">if</span> i &lt;= pivot]</span><br><span class="line">        greater = [i <span class="keyword">for</span> i <span class="keyword">in</span> arr[<span class="number">1</span>:] <span class="keyword">if</span> i &gt; pivot]</span><br><span class="line">        <span class="keyword">return</span> quick_sort(less) + [pivot] + quick_sort(greater)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(quick_sort([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>

<p>两个模型虽然生成的有细节上的区别，但是经过测试都是合理正确的快排算法。在这个问题上可以说表现一致。</p>
<p>(2) 生成检查有效数独算法</p>
<p>测试代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;path&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;path&quot;</span>, trust_remote_code=<span class="literal">True</span>, torch_dtype=torch.bfloat16).cuda()</span><br><span class="line">input_text = <span class="string">&quot;#write a function to test if sudoku is valid.&quot;</span></span><br><span class="line">inputs = tokenizer(input_text, return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line">outputs = model.generate(**inputs, max_length=<span class="number">512</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>

<p>CodeLlama-13b-hf 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#check if each row is valid</span></span><br><span class="line"><span class="comment">#check if each column is valid</span></span><br><span class="line"><span class="comment">#check if each 3x3 box is valid</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_valid</span>(<span class="params">board</span>):</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> board:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_row_valid(row):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(board[<span class="number">0</span>])):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_col_valid(board, col):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> box <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        <span class="keyword">for</span> box_row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> is_box_valid(board, box, box_row):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_row_valid</span>(<span class="params">row</span>):</span><br><span class="line">    row_set = <span class="built_in">set</span>(row)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(row_set) == <span class="number">9</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_col_valid</span>(<span class="params">board, col</span>):</span><br><span class="line">    col_set = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> board:</span><br><span class="line">        col_set.add(row[col])</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(col_set) == <span class="number">9</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_box_valid</span>(<span class="params">board, box, box_row</span>):</span><br><span class="line">    box_set = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(box*<span class="number">3</span>, box*<span class="number">3</span> + <span class="number">3</span>):</span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(box_row*<span class="number">3</span>, box_row*<span class="number">3</span> + <span class="number">3</span>):</span><br><span class="line">            box_set.add(board[row][col])</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(box_set) == <span class="number">9</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<p>StarCoder 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">is_valid</span>(<span class="params">board</span>):</span><br><span class="line">    <span class="comment">#check rows</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> board:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_valid_row(row):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="comment">#check columns</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_valid_col(board, col):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="comment">#check 3x3 boxesa</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">9</span>, <span class="number">3</span>):</span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">9</span>, <span class="number">3</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> is_valid_box(board, row, col):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#check if a row is valid</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_valid_row</span>(<span class="params">row</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(<span class="built_in">set</span>(row)) == <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#check if a column is valid</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_valid_col</span>(<span class="params">board, col</span>):</span><br><span class="line">    <span class="keyword">return</span> is_valid_row([row[col] <span class="keyword">for</span> row <span class="keyword">in</span> board])</span><br><span class="line"></span><br><span class="line"><span class="comment">#check if a 3x3 box is valid</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_valid_box</span>(<span class="params">board, row, col</span>):</span><br><span class="line">    <span class="keyword">return</span> is_valid_row([board[row+i][col+j] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br></pre></td></tr></table></figure>

<p>两个模型在算法代码题上的生成都很完整且正确。</p>
<ol>
<li><strong>代码补全</strong></li>
</ol>
<p>CodeLlama-13b-hf 模型的测试代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, pipeline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;path&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;path&quot;</span>, trust_remote_code=<span class="literal">True</span>, torch_dtype=torch.bfloat16).cuda()</span><br><span class="line"></span><br><span class="line">input_text = <span class="string">&quot;&quot;&quot;def quick_sort(arr):</span></span><br><span class="line"><span class="string">    if len(arr) &lt;= 1:</span></span><br><span class="line"><span class="string">        return arr</span></span><br><span class="line"><span class="string">    pivot = arr[0]</span></span><br><span class="line"><span class="string">    left = []</span></span><br><span class="line"><span class="string">    right = []</span></span><br><span class="line"><span class="string">&lt;FILL_ME&gt;</span></span><br><span class="line"><span class="string">        if arr[i] &lt; pivot:</span></span><br><span class="line"><span class="string">            left.append(arr[i])</span></span><br><span class="line"><span class="string">        else:</span></span><br><span class="line"><span class="string">            right.append(arr[i])</span></span><br><span class="line"><span class="string">    return quick_sort(left) + [pivot] + quick_sort(right)&quot;&quot;&quot;</span></span><br><span class="line">inputs = tokenizer(input_text, return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line">outputs = model.generate(**inputs, max_length=<span class="number">256</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)[(<span class="built_in">len</span>(input_text)-<span class="built_in">len</span>(<span class="string">&#x27;&lt;FILL_ME&gt;&#x27;</span>)):])</span><br></pre></td></tr></table></figure>
<p>CodeLlama-13b-hf 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(arr)):</span><br></pre></td></tr></table></figure>

<p>StarCoder 模型的测试代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;path&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;path&quot;</span>, trust_remote_code=<span class="literal">True</span>, torch_dtype=torch.bfloat16).cuda()</span><br><span class="line">input_text = <span class="string">&quot;&quot;&quot;&lt;fim_prefix&gt;def quick_sort(arr):</span></span><br><span class="line"><span class="string">    if len(arr) &lt;= 1:</span></span><br><span class="line"><span class="string">        return arr</span></span><br><span class="line"><span class="string">    pivot = arr[0]</span></span><br><span class="line"><span class="string">    left = []</span></span><br><span class="line"><span class="string">    right = []</span></span><br><span class="line"><span class="string">&lt;fim_suffix&gt;</span></span><br><span class="line"><span class="string">        if arr[i] &lt; pivot:</span></span><br><span class="line"><span class="string">            left.append(arr[i])</span></span><br><span class="line"><span class="string">        else:</span></span><br><span class="line"><span class="string">            right.append(arr[i])</span></span><br><span class="line"><span class="string">    return quick_sort(left) + [pivot] + quick_sort(right)&lt;fim_middle&gt;&quot;&quot;&quot;</span></span><br><span class="line">inputs = tokenizer(input_text, return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line">outputs = model.generate(**inputs, max_length=<span class="number">128</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)[(<span class="built_in">len</span>(input_text)-<span class="built_in">len</span>(<span class="string">&#x27;&lt;fim_prefix&gt;&#x27;</span>)-<span class="built_in">len</span>(<span class="string">&#x27;&lt;fim_suffix&gt;&#x27;</span>)-<span class="built_in">len</span>(<span class="string">&#x27;&lt;fim_middle&gt;&#x27;</span>)):])</span><br></pre></td></tr></table></figure>
<p>StarCoder 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(arr)):</span><br></pre></td></tr></table></figure>

<p>由于两个模型所使用的 special token 不相同，所以两份测试代码输入表示略有不同，两个模型对于快排算法补全的结果都是正确的。</p>
<ol>
<li><strong>生成单元测试</strong></li>
</ol>
<p>(1) 用英文注释作为 prompt 给快排函数生成单元测试</p>
<p>测试代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(path, trust_remote_code=<span class="literal">True</span>, torch_dtype=torch.bfloat16).cuda()</span><br><span class="line">input_text = <span class="string">&quot;&quot;&quot;#Generate one test case for the following code.</span></span><br><span class="line"><span class="string">def quick_sort(arr):</span></span><br><span class="line"><span class="string">    if len(arr) &lt; 2:</span></span><br><span class="line"><span class="string">        return arr</span></span><br><span class="line"><span class="string">    else:</span></span><br><span class="line"><span class="string">        pivot = arr[0]</span></span><br><span class="line"><span class="string">        less = [i for i in arr[1:] if i &lt;= pivot]</span></span><br><span class="line"><span class="string">        greater = [i for i in arr[1:] if i &gt; pivot]</span></span><br><span class="line"><span class="string">        return quick_sort(less) + [pivot] + quick_sort(greater)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">inputs = tokenizer(input_text, return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line">outputs = model.generate(**inputs, max_length=<span class="number">256</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)[<span class="built_in">len</span>(input_text):])</span><br></pre></td></tr></table></figure>
<p>CodeLlama-13b-hf 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_quick_sort</span>():</span><br><span class="line">    <span class="keyword">assert</span> quick_sort([]) == []</span><br><span class="line">    <span class="keyword">assert</span> quick_sort([<span class="number">1</span>]) == [<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">assert</span> quick_sort([<span class="number">1</span>, <span class="number">2</span>]) == [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">    <span class="keyword">assert</span> quick_sort([<span class="number">2</span>, <span class="number">1</span>]) == [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">    <span class="keyword">assert</span> quick_sort([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) == [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">    <span class="keyword">assert</span> quick_sort([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]) == [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">    <span class="keyword">assert</span> quick_sort([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>]) == [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">    <span class="keyword">assert</span> quick_sort([</span><br></pre></td></tr></table></figure>

<p>StarCoder 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Test</span></span><br><span class="line"><span class="built_in">print</span>(quick_sort([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]))</span><br></pre></td></tr></table></figure>

<p>CodeLlama-13b-Python-hf 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">import</span> random</span><br><span class="line">    <span class="keyword">import</span> pprint</span><br><span class="line">    t = [random.randint(<span class="number">1</span>, <span class="number">100</span>) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;origin array:&quot;</span>)</span><br><span class="line">    pprint.pprint(t)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;sorted array:&quot;</span>)</span><br><span class="line">    pprint.pprint(quick_sort(t))</span><br></pre></td></tr></table></figure>

<p>在单元测试生成问题上，StarCoder 成功理解了“只生成一个测试用例”中的“一个”，而 CodeLlama-13b 试图在设定的输出长度里持续输出多个测试用例。在这个测试中，StarCoder 表现出了比 CodeLlama 更好的对自然语言（英语）的理解能力。但同时我们也注意到，StarCoder 生成的测试用例不如 CodeLlama-13b-Python 合理，该问题是一个排序算法，立项的生成结果应该为乱序的数组更为合理，而 CodeLlama-13b-Python 首先生成了乱序的数组，再对快排算法进行结果测试。</p>
<p>我们接下来测试了用中文进行 prompt 的结果。</p>
<p>(2) 用中文注释作为 prompt 给快排函数生成单元测试</p>
<p>测试代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(path, trust_remote_code=<span class="literal">True</span>, torch_dtype=torch.bfloat16).cuda()</span><br><span class="line">input_text = <span class="string">&quot;&quot;&quot;#为下列代码生成一个测试用例。</span></span><br><span class="line"><span class="string">def quick_sort(arr):</span></span><br><span class="line"><span class="string">    if len(arr) &lt; 2:</span></span><br><span class="line"><span class="string">        return arr</span></span><br><span class="line"><span class="string">    else:</span></span><br><span class="line"><span class="string">        pivot = arr[0]</span></span><br><span class="line"><span class="string">        less = [i for i in arr[1:] if i &lt;= pivot]</span></span><br><span class="line"><span class="string">        greater = [i for i in arr[1:] if i &gt; pivot]</span></span><br><span class="line"><span class="string">        return quick_sort(less) + [pivot] + quick_sort(greater)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">inputs = tokenizer(input_text, return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line">outputs = model.generate(**inputs, max_length=<span class="number">512</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)[<span class="built_in">len</span>(input_text):])</span><br></pre></td></tr></table></figure>

<p>CodeLlama-13b-hf 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#测试用例</span></span><br><span class="line"><span class="keyword">import</span> unittest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TestQuickSort</span>(unittest.TestCase):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_quick_sort</span>(<span class="params">self</span>):</span><br><span class="line">        self.assertEqual(quick_sort([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]), [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">        self.assertEqual(quick_sort([<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]), [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">        self.assertEqual(quick_sort([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]), [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">        self.assertEqual(quick_sort([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]), [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    unittest.main()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行用时 :12 ms, 在所有 Python3 提交中击败了99.98%的用户</span></span><br><span class="line"><span class="comment"># 内存消耗 :13.8 MB, 在所有 Python3 提交中击败了100%的用户</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 作者：LeetCode-Solution</span></span><br><span class="line"><span class="comment"># 链接：</span></span><br></pre></td></tr></table></figure>

<p>StarCoder 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(quick_sort([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<p>CodeLlama-13b-Python-hf 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">import</span> random</span><br><span class="line">    <span class="keyword">import</span> pprint</span><br><span class="line">    t = [random.randint(<span class="number">1</span>, <span class="number">100</span>) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;origin array:&quot;</span>)</span><br><span class="line">    pprint.pprint(t)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;sorted array:&quot;</span>)</span><br><span class="line">    pprint.pprint(quick_sort(t))</span><br></pre></td></tr></table></figure>

<p>可以从 CodeLlama-13b-hf 的输出发现它极度依赖于用于训练的原始数据，用户的隐私信息也没有做合适的处理；而相比之下，专门处理过 PII Redaction 的 StarCoder 模型就不太会出现这种情况。</p>
<p>除此之外，在 StarCoder 的生成中，中文 prompt 生成的测试用例比英文 prompt 生成的测试用例更为合理——用于进行测试的数组应为乱序数组。</p>
<p>CodeLlama-13b-Python 依旧是生成单元测试最合理的模型，比 CodeLlama-13b 生成的结果合理、完整很多，这说明在通用语言模型上训练而成的 CodeLlama 十分需要专业化的数据对模型专业化生成能力进行提升。</p>
<p>(3) 给打印“Hello world!”函数生成单元测试</p>
<p>测试代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(path, trust_remote_code=<span class="literal">True</span>, torch_dtype=torch.bfloat16).cuda()</span><br><span class="line">input_text = <span class="string">&quot;&quot;&quot;#Complete unit test for the following code.</span></span><br><span class="line"><span class="string">def helloworld():</span></span><br><span class="line"><span class="string">    print(&quot;Hello world!&quot;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">inputs = tokenizer(input_text, return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line">outputs = model.generate(**inputs, max_length=<span class="number">256</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)[<span class="built_in">len</span>(input_text):])</span><br></pre></td></tr></table></figure>

<p>CodeLlama-13b-hf 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">helloworld2</span>(<span class="params">s: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Hello world!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">helloworld3</span>(<span class="params">s: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Hello world!&quot;</span>)</span><br><span class="line">...</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">helloworld7</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Hello world!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">helloworld8</span>():</span><br></pre></td></tr></table></figure>

<p>StarCoder 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_helloworld</span>():</span><br><span class="line">    <span class="keyword">assert</span> helloworld() == <span class="string">&quot;Hello world!&quot;</span></span><br></pre></td></tr></table></figure>

<p>CodeLlama-13b-Python-hf 模型的生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_helloworld</span>():</span><br><span class="line">    <span class="keyword">assert</span> helloworld() == <span class="string">&quot;Hello world!&quot;</span></span><br></pre></td></tr></table></figure>

<p>可以看到 StarCoder 模型和 CodeLlama-13b-Python 模型生成的代码较 CodeLlama-13b 更为合理。这进一步说明了 CodeLlama 对更专业化的数据十分 data-hungry。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h1><p><strong>1. StarCoder: 最先进的代码大模型</strong></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/StarCoder">https://huggingface.co/blog/zh/StarCoder</a></p>
<p><strong>2. StarCoder: A State-of-the-Art LLM for Code</strong></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/StarCoder">https://huggingface.co/blog/StarCoder</a></p>
<p><strong>3. Creating a Coding Assistant with StarCoder</strong></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/starchat-alpha">https://huggingface.co/blog/starchat-alpha</a></p>
<p><strong>4. 使用 StarCoder 创建一个编程助手</strong></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/starchat-alpha">https://huggingface.co/blog/zh/starchat-alpha</a></p>
<p><strong>5. StarCoder Memorization Experiment Highlights Privacy Risks of Fine-Tuning On Code</strong></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/dhuynh95/StarCoder-memorization-experiment">https://huggingface.co/blog/dhuynh95/StarCoder-memorization-experiment</a></p>
<p><strong>6. Code Llama: Open Foundation Models for Code</strong></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.12950">https://arxiv.org/abs/2308.12950</a></p>
<p><strong>7. StarCoder: may the source be with you!</strong></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.06161v2">https://arxiv.org/abs/2305.06161v2</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Amelia Yin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/01/11/240111/">http://example.com/2024/01/11/240111/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Amelia</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/CodeG/">CodeG</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/2024/05/17/hello-world/" title="Hello World"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Hello World</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Amelia Yin</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ameliayinn" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:ameliayin@stu.pku.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome to my blog.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3-StarCoder-%E6%A8%A1%E5%9E%8B"><span class="toc-text">深入了解 StarCoder 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#01-%E8%A7%A3%E5%AF%86-StarCoder-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E5%92%8C%E6%B8%85%E6%B4%97"><span class="toc-text">01 解密 StarCoder 预训练数据管理和清洗</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86"><span class="toc-text">训练集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%80%89%E6%8B%A9"><span class="toc-text">数据选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%BB%E9%87%8D%E5%A4%84%E7%90%86"><span class="toc-text">去重处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%9D%83%E9%87%8D"><span class="toc-text">数据集的权重</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#02-StarCoder-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E5%92%8C%E6%8A%80%E5%B7%A7"><span class="toc-text">02 StarCoder 模型训练方法和技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="toc-text">数据格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">分词器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6"><span class="toc-text">模型框架</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#03-StarCoder-%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B%E8%AF%A6%E8%A7%A3"><span class="toc-text">03 StarCoder 模型评测详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#StarCoder-Python-Evaluation"><span class="toc-text">StarCoder: Python Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#StarCoder-and-StarCoderBase-Multi-Language-Evaluation"><span class="toc-text">StarCoder and StarCoderBase: Multi-Language Evaluation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#04-StarCoder-%E5%A6%82%E4%BD%95%E4%BF%9D%E9%9A%9C%E6%95%B0%E6%8D%AE%E9%9A%90%E7%A7%81%E5%AE%89%E5%85%A8%EF%BC%9F"><span class="toc-text">04 StarCoder 如何保障数据隐私安全？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86"><span class="toc-text">数据收集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%BB%84%E6%88%90"><span class="toc-text">数据组成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8C%BA%E5%88%86-PII"><span class="toc-text">区分 PII</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0"><span class="toc-text">质量评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#StarEncoder"><span class="toc-text">StarEncoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PII-%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">PII 检测模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PII-%E5%8D%A0%E4%BD%8D%E7%AC%A6"><span class="toc-text">PII 占位符</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3-Code-Llama-%E6%A8%A1%E5%9E%8B"><span class="toc-text">深入了解 Code Llama 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#01-%E8%A7%A3%E5%AF%86-CodeLlama-%E5%BE%AE%E8%B0%83%E6%96%B9%E6%A1%88"><span class="toc-text">01 解密 CodeLlama 微调方案</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A1%AB%E5%85%85"><span class="toc-text">填充</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%95%BF%E6%96%87%E6%9C%AC%E5%BE%AE%E8%B0%83"><span class="toc-text">长文本微调</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83"><span class="toc-text">指令微调</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#02-CodeLlama-%E7%9A%84%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82%E8%AF%A6%E8%A7%A3"><span class="toc-text">02 CodeLlama 的训练细节详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-text">优化参数设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E5%BE%AE%E8%B0%83-Long-context-fine-tuning-LCFT"><span class="toc-text">长上下文微调 (Long context fine-tuning, LCFT)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#03-CodeLlama-%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B"><span class="toc-text">03 CodeLlama 模型评测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#04-CodeLlama-%E5%A6%82%E4%BD%95%E4%BF%9D%E9%9A%9C%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%EF%BC%9F"><span class="toc-text">04 CodeLlama 如何保障模型安全？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#StarCoder-%E5%92%8C-CodeLlama-%E8%B0%81%E6%9B%B4%E9%80%82%E5%90%88%E4%BD%9C%E4%B8%BA%E5%9F%BA%E5%BA%A7%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-text">StarCoder 和 CodeLlama 谁更适合作为基座模型？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#01-StarCoder-%E4%B8%8E-CodeLlama-%E5%9C%A8-HumanEval-benchmark-%E4%B8%8A%E7%9A%84%E8%AF%84%E6%B5%8B%E5%AF%B9%E6%AF%94"><span class="toc-text">01 StarCoder 与 CodeLlama 在 HumanEval benchmark 上的评测对比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#02-StarCoder-%E4%B8%8E-CodeLlama-%E5%9C%A8%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E4%B8%AD%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">02 StarCoder 与 CodeLlama 在实际应用中的比较</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-text">参考链接</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/05/17/hello-world/" title="Hello World">Hello World</a><time datetime="2024-05-17T08:23:53.984Z" title="发表于 2024-05-17 16:23:53">2024-05-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/01/11/240111/" title="深度解读代码生成模型 StarCoder VS CodeLlama">深度解读代码生成模型 StarCoder VS CodeLlama</a><time datetime="2024-01-10T16:00:00.000Z" title="发表于 2024-01-11 00:00:00">2024-01-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Amelia Yin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>